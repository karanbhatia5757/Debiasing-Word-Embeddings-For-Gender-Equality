{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing Non-Gender Specific Words for Gender Equality.\n",
    "This Ipython Notebook contains code to remove bias from Word Vectors of Non-Gender Specific Words like computer, engineer, fashion, doctor etc.<br>\n",
    "Gender bias is not a healthy practice, neither in real life and should not be there in Natural Language Processing Systems.<br>\n",
    "Ideas here are implemented from a paper by Boliukbasi et al. (https://arxiv.org/abs/1607.06520).<br>\n",
    "The GloVe word embeddings are taken from Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing important Libraries.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function returns words and python map/dictionary containing word embeddings.\n",
    "def glove_vecs(glove_file):\n",
    "    \"\"\"Function Parameters: Path to glove vector.txt file\"\"\"\n",
    "    # Open Text file in read mode \n",
    "    with open(glove_file, 'r') as f:\n",
    "        # Creating empty set of words\n",
    "        words = set()\n",
    "        # Creating empty dictionary to store word:vector pair\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        # Reading file line by line\n",
    "        # Each line is a string with space in between.\n",
    "        for line in f:\n",
    "            # Strip() function strips the string from both starting and end.\n",
    "            # Split() function splits the string in separate elements around a space\n",
    "            line = line.strip().split()\n",
    "            # First element is the word \n",
    "            curr_word = line[0]\n",
    "            # Adding the word to the set words.\n",
    "            words.add(curr_word)\n",
    "            # Adding current word and it's vector in the dictionary.\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words, word_to_vec_map = glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding cosine similarity between 2 vectors.\n",
    "def cosine_similarity(a , b ):\n",
    "    \"\"\"Function Parameters: a , b are 2 different vectors whose cosine similarity is to be found.\"\"\"\n",
    "    \n",
    "    cos_similarity = np.dot(a , b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        \n",
    "    return cos_similarity    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding distance between 2 vectors using L-2 Norm.\n",
    "def L2distance(a , b):\n",
    "    \"\"\"Function Parameters: a , b are 2 different vectors whose cosine similarity is to be found.\"\"\"\n",
    "    \n",
    "    l2_distance = np.linalg.norm(a - b)\n",
    "    \n",
    "    return l2_distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can use any of the above 2 methods to find similarity between 2 vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Word Analogy:\n",
    "The function below finds the best possible word to complete an analogy, given 3 words.<br>\n",
    "eg: man:woman :: boy: ?. Find the best word from the dictionary that can fit in place of '?'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_analogy(a, b, c, word_to_vec_map):\n",
    "    \"\"\"Function Parameters: a , b , c are 3 words\n",
    "       words_to_vec_map: dictionary of word vectors.\"\"\"\n",
    "    \n",
    "    # Converting a,b and c to lower case.\n",
    "    a = a.lower()\n",
    "    b = b.lower()\n",
    "    c = c.lower()\n",
    "    \n",
    "    # Finding the vectors for the given words:\n",
    "    \n",
    "    a_vec = word_to_vec_map[a]\n",
    "    b_vec = word_to_vec_map[b]\n",
    "    c_vec = word_to_vec_map[c]\n",
    "    \n",
    "    # Getting all the words from the dictionary.\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    \n",
    "    # Setting maximum cosine similarity to large negative number.\n",
    "    \n",
    "    maximum_cosine_similarity = -500\n",
    "    \n",
    "    # Looping over all the words to find the best fit for the analogy.\n",
    "    \n",
    "    for w in words:\n",
    "        \n",
    "        # Skip a , b and c\n",
    "        if w in [a,b,c]:\n",
    "            continue\n",
    "            \n",
    "        cos_similarity = cosine_similarity(b_vec - a_vec , word_to_vec_map[w] - c_vec ) \n",
    "        \n",
    "        if cos_similarity > maximum_cosine_similarity:\n",
    "            # Overiting maximum_cosine_similarity \n",
    "            maximum_cosine_similarity = cos_similarity\n",
    "            # Saving the best word giving maximum_cosine_similarity\n",
    "            best_word = w\n",
    "    \n",
    "    \n",
    "    return best_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'american'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_analogy('italy', 'italian', 'america', word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Debiasing Non-Gender Specific Word Vectors.\n",
    "How a word vector can be biased? Let us see the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = word_to_vec_map['woman'] - word_to_vec_map['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.312447968503\n",
      "-0.165662998616\n",
      "0.315597935396\n",
      "0.17632041839\n",
      "We see that male names have negative similarity and female names have positive similarity. That's OK because\n",
      "the vector x is woman - man\n"
     ]
    }
   ],
   "source": [
    "# Let us see similarity between some gender specific names and the vector'x'\n",
    "names = ['ronaldo' , 'jack', 'marie' , 'priya']\n",
    "for name in names:\n",
    "    print(cosine_similarity(word_to_vec_map[name], x))\n",
    "\n",
    "print(\"We see that male names have negative similarity and female names have positive similarity. That's OK because\\nthe vector x is woman - man\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.131937324476\n",
      "-0.0803928049452\n",
      "0.118952894109\n",
      "0.0236297984509\n",
      "0.384601436374\n",
      "0.0647250443346\n",
      "We see that words like technology , engineer are inclined towards man while literature is inclined towards woman.\n"
     ]
    }
   ],
   "source": [
    "# Let us see similarity between some words that should be non-gender specific.\n",
    "common_words = ['technology' , 'engineer' , 'doctor','grandfather','grandmother','literature']\n",
    "for word in common_words:\n",
    "    print(cosine_similarity(word_to_vec_map[word], x))\n",
    "\n",
    "print(\"We see that words like technology , engineer are inclined towards man while literature is inclined towards woman.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutralizing Non-Gender Specific Words.\n",
    "To neutralize non-gender specific words, we need to have 0 similarity between the word from both man and woman. In other words, cosine similarity between the word and 'x' obtained above should be nearly 0.<br>\n",
    "For this, we need to find the bias direction from the vector of the word and then subtract this from original word vector. \n",
    "# Bias Direction:\n",
    "Bias direction is given by projection of the word vector 'w' onto the direction of vector x.\n",
    "bias_direction = w . x/|x|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neutralize(word , x , word_to_vec_map):\n",
    "    # Extracting word vector from the dictionary.\n",
    "    w = word_to_vec_map[word]\n",
    "    \n",
    "    # Finding the bias direction\n",
    "    \n",
    "    bias_direction = np.dot(w,x) * x /np.square((np.linalg.norm(x)))\n",
    "    \n",
    "    w_unbiased = w - bias_direction\n",
    "    \n",
    "    return w_unbiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between literature and x, before neutralizing:  0.0647250443346\n",
      "cosine similarity between literature and x, after neutralizing:  -3.12308857172e-17\n"
     ]
    }
   ],
   "source": [
    "w = \"literature\"\n",
    "print(\"cosine similarity between \" + w + \" and x, before neutralizing: \", cosine_similarity(word_to_vec_map[\"literature\"], x))\n",
    "\n",
    "e_unbiased = neutralize(\"literature\", x, word_to_vec_map)\n",
    "print(\"cosine similarity between \" + w + \" and x, after neutralizing: \", cosine_similarity(e_unbiased, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equalizing Word Pairs:\n",
    "We want word pairs like (father, mother) , (actor,actress) etc. to be equidistant from the words we neutralized above. Or in other words, these words should be equidistant from the non-bias axis/direction.<br>\n",
    "In the function below, we apply the equalizing algorithms given by Boliukbasi et al. (https://arxiv.org/abs/1607.06520)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def equalize(w1, w2, bias_axis, word_to_vec_map):\n",
    "    # Extracting vectors from dictionary.\n",
    "    \n",
    "    w1_vec = word_to_vec_map[w1]\n",
    "    \n",
    "    w2_vec = word_to_vec_map[w2]\n",
    "    \n",
    "    # The equations implemented below are described in the paper in the given link.\n",
    "    mu = (w1_vec + w2_vec) / 2\n",
    "    \n",
    "    # Projection of mu over bias_axis and the orthogonal axis.\n",
    "    mu_B = np.dot(mu,bias_axis) * bias_axis / np.square(np.linalg.norm(bias_axis))\n",
    "    mu_orth = mu - mu_B\n",
    "    \n",
    "    w1_vecB = np.dot(w1_vec,bias_axis) * bias_axis / np.square(np.linalg.norm(bias_axis))\n",
    "    w2_vecB = np.dot(w2_vec,bias_axis) * bias_axis / np.square(np.linalg.norm(bias_axis))\n",
    "    \n",
    "    w1_vecB_corrected = np.sqrt(np.absolute(1 - np.square(np.linalg.norm(mu_orth)))) * (w1_vecB - mu_B) / np.absolute((w1_vec - mu_orth) - mu_B) \n",
    "    w2_vecB_corrected = np.sqrt(np.absolute(1 - np.square(np.linalg.norm(mu_orth)))) * (w2_vecB - mu_B) / np.absolute((w2_vec - mu_orth) - mu_B)\n",
    "    \n",
    "    e1 = w1_vecB_corrected  + mu_orth\n",
    "    e2 = w2_vecB_corrected  + mu_orth\n",
    "    \n",
    "    return e1 , e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"man\"], gender) =  -0.117110957653\n",
      "cosine_similarity(word_to_vec_map[\"woman\"], gender) =  0.356666188463\n",
      "\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  -0.716572752584\n",
      "cosine_similarity(e2, gender) =  0.739659647493\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], x))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], x))\n",
    "print()\n",
    "e1, e2 = equalize(\"man\", \"woman\", x, word_to_vec_map)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, x))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
